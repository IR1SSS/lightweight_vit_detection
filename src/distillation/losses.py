"""
Distillation Loss Functions for Knowledge Distillation.

This module implements various distillation loss functions including:
- Response distillation (soft labels)
- Feature distillation (intermediate features)
- Relation distillation (feature relationships)
"""

from typing import List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


class ResponseDistillationLoss(nn.Module):
    """
    Response-based Knowledge Distillation Loss.
    
    Transfers knowledge from teacher to student using soft labels
    generated by the teacher's output logits.
    
    Reference:
        Hinton et al. "Distilling the Knowledge in a Neural Network"
    """
    
    def __init__(
        self,
        temperature: float = 4.0,
        reduction: str = 'mean'
    ) -> None:
        """
        Initialize response distillation loss.
        
        Args:
            temperature: Softmax temperature for soft labels.
                        Higher temperature produces softer probability distributions.
            reduction: Reduction method ('mean', 'sum', 'none').
        """
        super().__init__()
        self.temperature = temperature
        self.reduction = reduction
        
    def forward(
        self,
        student_logits: List[torch.Tensor],
        teacher_logits: List[torch.Tensor]
    ) -> torch.Tensor:
        """
        Compute response distillation loss.
        
        Args:
            student_logits: List of student output tensors.
            teacher_logits: List of teacher output tensors.
            
        Returns:
            Distillation loss value.
        """
        if not student_logits or not teacher_logits:
            return torch.tensor(0.0)
            
        total_loss = 0.0
        
        for s_logits, t_logits in zip(student_logits, teacher_logits):
            # Flatten spatial dimensions
            B = s_logits.shape[0]
            s_flat = s_logits.view(B, -1)
            t_flat = t_logits.view(B, -1)
            
            # Compute soft targets
            soft_target = F.softmax(t_flat / self.temperature, dim=1)
            soft_pred = F.log_softmax(s_flat / self.temperature, dim=1)
            
            # KL divergence loss
            loss = F.kl_div(
                soft_pred, soft_target, reduction='batchmean'
            ) * (self.temperature ** 2)
            
            total_loss = total_loss + loss
            
        return total_loss / len(student_logits)


class FeatureDistillationLoss(nn.Module):
    """
    Feature-based Knowledge Distillation Loss.
    
    Transfers knowledge by aligning intermediate feature representations
    between teacher and student networks.
    
    Supports:
    - L2 loss (MSE)
    - L1 loss
    - Cosine similarity loss
    """
    
    def __init__(
        self,
        loss_type: str = 'mse',
        normalize: bool = True,
        align_channels: bool = True
    ) -> None:
        """
        Initialize feature distillation loss.
        
        Args:
            loss_type: Type of loss ('mse', 'l1', 'cosine').
            normalize: Whether to normalize features before computing loss.
            align_channels: Whether to align channel dimensions.
        """
        super().__init__()
        self.loss_type = loss_type
        self.normalize = normalize
        self.align_channels = align_channels
        
        # Channel alignment layers (created dynamically)
        self.align_layers = nn.ModuleDict()
        
    def forward(
        self,
        student_feat: torch.Tensor,
        teacher_feat: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute feature distillation loss.
        
        Args:
            student_feat: Student feature tensor [B, C_s, H, W].
            teacher_feat: Teacher feature tensor [B, C_t, H, W].
            
        Returns:
            Feature distillation loss value.
        """
        # Handle spatial size mismatch
        if student_feat.shape[2:] != teacher_feat.shape[2:]:
            student_feat = F.adaptive_avg_pool2d(
                student_feat, teacher_feat.shape[2:]
            )
            
        # Handle channel mismatch
        if student_feat.shape[1] != teacher_feat.shape[1]:
            # Create alignment layer if needed
            key = f"{student_feat.shape[1]}_{teacher_feat.shape[1]}"
            if key not in self.align_layers:
                self.align_layers[key] = nn.Conv2d(
                    student_feat.shape[1], 
                    teacher_feat.shape[1], 
                    1, bias=False
                ).to(student_feat.device)
            student_feat = self.align_layers[key](student_feat)
            
        # Normalize features
        if self.normalize:
            student_feat = F.normalize(student_feat, dim=1)
            teacher_feat = F.normalize(teacher_feat, dim=1)
            
        # Compute loss
        if self.loss_type == 'mse':
            loss = F.mse_loss(student_feat, teacher_feat)
        elif self.loss_type == 'l1':
            loss = F.l1_loss(student_feat, teacher_feat)
        elif self.loss_type == 'cosine':
            loss = 1 - F.cosine_similarity(
                student_feat.flatten(2), 
                teacher_feat.flatten(2), 
                dim=2
            ).mean()
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")
            
        return loss


class RelationDistillationLoss(nn.Module):
    """
    Relation-based Knowledge Distillation Loss.
    
    Transfers structural knowledge by matching relationships
    between samples in teacher and student feature spaces.
    
    Reference:
        Park et al. "Relational Knowledge Distillation"
    """
    
    def __init__(
        self,
        distance_type: str = 'euclidean',
        use_angle: bool = True
    ) -> None:
        """
        Initialize relation distillation loss.
        
        Args:
            distance_type: Type of distance metric ('euclidean', 'cosine').
            use_angle: Whether to use angle-wise distillation.
        """
        super().__init__()
        self.distance_type = distance_type
        self.use_angle = use_angle
        
    def forward(
        self,
        student_feat: torch.Tensor,
        teacher_feat: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute relation distillation loss.
        
        Args:
            student_feat: Student feature tensor.
            teacher_feat: Teacher feature tensor.
            
        Returns:
            Relation distillation loss value.
        """
        # Flatten features
        student_flat = student_feat.view(student_feat.size(0), -1)
        teacher_flat = teacher_feat.view(teacher_feat.size(0), -1)
        
        # Handle dimension mismatch
        if student_flat.shape[1] != teacher_flat.shape[1]:
            min_dim = min(student_flat.shape[1], teacher_flat.shape[1])
            student_flat = student_flat[:, :min_dim]
            teacher_flat = teacher_flat[:, :min_dim]
        
        # Compute pairwise distances
        student_dist = self._pairwise_distance(student_flat)
        teacher_dist = self._pairwise_distance(teacher_flat)
        
        # Distance-wise distillation
        loss = F.smooth_l1_loss(student_dist, teacher_dist)
        
        # Angle-wise distillation
        if self.use_angle:
            student_angle = self._pairwise_angle(student_flat)
            teacher_angle = self._pairwise_angle(teacher_flat)
            loss = loss + F.smooth_l1_loss(student_angle, teacher_angle)
            
        return loss
        
    def _pairwise_distance(self, x: torch.Tensor) -> torch.Tensor:
        """Compute pairwise Euclidean distances."""
        x_norm = (x ** 2).sum(1).view(-1, 1)
        dist = x_norm + x_norm.t() - 2.0 * torch.mm(x, x.t())
        dist = torch.clamp(dist, min=0).sqrt()
        return dist / (dist.max() + 1e-8)  # Normalize
        
    def _pairwise_angle(self, x: torch.Tensor) -> torch.Tensor:
        """Compute pairwise cosine angles."""
        x_norm = F.normalize(x, dim=1)
        return torch.mm(x_norm, x_norm.t())


class CombinedDistillationLoss(nn.Module):
    """
    Combined distillation loss with multiple components.
    
    Combines response, feature, and relation distillation losses
    with configurable weights.
    """
    
    def __init__(
        self,
        response_weight: float = 1.0,
        feature_weight: float = 0.5,
        relation_weight: float = 0.1,
        temperature: float = 4.0
    ) -> None:
        """
        Initialize combined distillation loss.
        
        Args:
            response_weight: Weight for response distillation.
            feature_weight: Weight for feature distillation.
            relation_weight: Weight for relation distillation.
            temperature: Temperature for soft labels.
        """
        super().__init__()
        
        self.response_weight = response_weight
        self.feature_weight = feature_weight
        self.relation_weight = relation_weight
        
        self.response_loss = ResponseDistillationLoss(temperature)
        self.feature_loss = FeatureDistillationLoss()
        self.relation_loss = RelationDistillationLoss()
        
    def forward(
        self,
        student_outputs: dict,
        teacher_outputs: dict,
        student_features: Optional[List[torch.Tensor]] = None,
        teacher_features: Optional[List[torch.Tensor]] = None
    ) -> dict:
        """
        Compute combined distillation loss.
        
        Args:
            student_outputs: Student model outputs.
            teacher_outputs: Teacher model outputs.
            student_features: Optional list of student intermediate features.
            teacher_features: Optional list of teacher intermediate features.
            
        Returns:
            Dictionary of individual and total losses.
        """
        losses = {}
        total_loss = 0.0
        
        # Response distillation
        if self.response_weight > 0:
            response = self.response_loss(
                student_outputs.get('cls_scores', []),
                teacher_outputs.get('cls_scores', [])
            )
            losses['loss_response'] = response
            total_loss = total_loss + self.response_weight * response
            
        # Feature distillation
        if self.feature_weight > 0 and student_features and teacher_features:
            feature_loss = 0.0
            for s_feat, t_feat in zip(student_features, teacher_features):
                feature_loss = feature_loss + self.feature_loss(s_feat, t_feat)
            feature_loss = feature_loss / len(student_features)
            losses['loss_feature'] = feature_loss
            total_loss = total_loss + self.feature_weight * feature_loss
            
        # Relation distillation
        if self.relation_weight > 0 and student_features and teacher_features:
            relation = self.relation_loss(
                student_features[-1], teacher_features[-1]
            )
            losses['loss_relation'] = relation
            total_loss = total_loss + self.relation_weight * relation
            
        losses['loss_distill_total'] = total_loss
        
        return losses


class AttentionDistillationLoss(nn.Module):
    """
    Attention-based Knowledge Distillation Loss.
    
    Transfers knowledge by matching attention maps between
    teacher and student networks.
    
    Reference:
        Zagoruyko & Komodakis "Paying More Attention to Attention"
    """
    
    def __init__(self, p: int = 2) -> None:
        """
        Initialize attention distillation loss.
        
        Args:
            p: Power for attention map computation.
        """
        super().__init__()
        self.p = p
        
    def forward(
        self,
        student_feat: torch.Tensor,
        teacher_feat: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute attention distillation loss.
        
        Args:
            student_feat: Student feature tensor [B, C, H, W].
            teacher_feat: Teacher feature tensor [B, C, H, W].
            
        Returns:
            Attention distillation loss value.
        """
        # Compute attention maps (channel-wise sum of squared activations)
        student_att = self._attention_map(student_feat)
        teacher_att = self._attention_map(teacher_feat)
        
        # Handle spatial size mismatch
        if student_att.shape != teacher_att.shape:
            student_att = F.interpolate(
                student_att.unsqueeze(1),
                size=teacher_att.shape[1:],
                mode='bilinear',
                align_corners=False
            ).squeeze(1)
            
        # L2 loss on normalized attention maps
        return F.mse_loss(student_att, teacher_att)
        
    def _attention_map(self, feat: torch.Tensor) -> torch.Tensor:
        """Compute spatial attention map from feature tensor."""
        # Sum of absolute values raised to power p, across channels
        att = torch.pow(torch.abs(feat), self.p).sum(dim=1)
        # Normalize
        att = F.normalize(att.view(att.size(0), -1), dim=1)
        return att.view(att.size(0), feat.size(2), feat.size(3))
